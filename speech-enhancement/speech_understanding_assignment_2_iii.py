# -*- coding: utf-8 -*-
"""Speech-Understanding-Assignment-2-III.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D3-_5a1aKSLGR4HihSB5fa3wWFyK53o1

# MOUNTING GOOGLE DRIVE
"""

#!fusermount -u /content/drive
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

"""# DATASET PREPARATION"""

import os
import random
import torchaudio
import torchaudio.transforms as transforms
import librosa
import numpy as np
import soundfile as sf

# Set paths

VOXCELEB2_DIR = "/content/drive/My Drive/vox2_test_aac/aac"  # Change to your dataset directory
OUTPUT_DIR = "/content/drive/My Drive/vox2_test_aac/mix_utterances"  # Change to where you want to save mixed files

speaker_ids = os.listdir(VOXCELEB2_DIR)

# Select speakers
train_speakers = speaker_ids[:50]  # First 4 for training
test_speakers = speaker_ids[50:]  # Last 2 for testing

# Function to load and resample audio
def load_audio(file_path, target_sr=16000):
    y, sr = librosa.load(file_path, sr=None)
    if sr != target_sr:
        y = librosa.resample(y, orig_sr=sr, target_sr=target_sr)
    return y, target_sr

# Function to create mixed audio and save all three files
def create_mixture(spk1_file, spk2_file, spk1_id, spk2_id, output_base):
    y1, sr1 = load_audio(spk1_file)
    y2, sr2 = load_audio(spk2_file)

    # Make sure both signals are the same length
    min_length = min(len(y1), len(y2))
    y1, y2 = y1[:min_length], y2[:min_length]

    # Normalize
    y1 = y1 / np.max(np.abs(y1))
    y2 = y2 / np.max(np.abs(y2))
    mixture = (y1 + y2) / 2  # Mixed signal

    # Create output folder
    folder_name = f"{spk1_id}-{spk2_id}"
    folder_path = os.path.join(output_base, folder_name)
    os.makedirs(folder_path, exist_ok=True)

    # Save mixture
    mix_file = os.path.join(folder_path, f"{folder_name}.wav")
    sf.write(mix_file, mixture, sr1)

    # Save individual speaker files
    spk1_file_out = os.path.join(folder_path, f"{spk1_id}.wav")
    spk2_file_out = os.path.join(folder_path, f"{spk2_id}.wav")
    sf.write(spk1_file_out, y1, sr1)
    sf.write(spk2_file_out, y2, sr2)
    # spk1_ref_out = os.path.join(folder_path, f"{spk1_id}_ref.wav")
    # spk2_ref_out = os.path.join(folder_path, f"{spk2_id}_ref.wav")
    # sf.write(spk1_ref_out, y1, sr1)
    # sf.write(spk2_ref_out, y2, sr2)

    print(f"Saved: {folder_path}")

# Function to create dataset
def create_dataset(speaker_list, num_mixtures, dataset_type):
    output_base = os.path.join(OUTPUT_DIR, dataset_type)
    print(output_base)
    os.makedirs(output_base, exist_ok=True)
    print('inside create datasert')
    for _ in range(num_mixtures):
        spk1, spk2 = random.sample(speaker_list, 2)  # Randomly select 2 speakers
        # print(spk1, spk2)
        spk1_folder = os.path.join(VOXCELEB2_DIR, spk1)
        spk2_folder = os.path.join(VOXCELEB2_DIR, spk2)

        # Select random video session
        spk1_videos = os.listdir(spk1_folder)
        spk2_videos = os.listdir(spk2_folder)
        if not spk1_videos or not spk2_videos:
          continue

        vid1 = random.choice(spk1_videos)
        vid2 = random.choice(spk2_videos)
        spk1_files = [f for f in os.listdir(os.path.join(spk1_folder, vid1)) if f.endswith(".wav")]
        spk2_files = [f for f in os.listdir(os.path.join(spk2_folder, vid2)) if f.endswith(".wav")]
        if not spk1_files or not spk2_files:
            continue
        # print(vid1, spk1_files)
        # print(vid2, spk2_files)
        file1 = os.path.join(spk1_folder, vid1, random.choice(spk1_files))
        file2 = os.path.join(spk2_folder, vid2, random.choice(spk2_files))
        create_mixture(file1, file2, spk1, spk2, output_base)

# Create Training and Testing Datasets
create_dataset(train_speakers, num_mixtures=300, dataset_type="train")
create_dataset(test_speakers, num_mixtures=100, dataset_type="test")



"""# SPEAKER SEPARATION"""
import os
import torch
import torchaudio
import pandas as pd
import numpy as np
from pesq import pesq
from torchaudio.transforms import Resample
from speechbrain.inference.separation import SepformerSeparation
import mir_eval

def load_model():
    model = SepformerSeparation.from_hparams(
        source="speechbrain/sepformer-wsj02mix",
        savedir='pretrained_models/sepformer-wsj02mix'
    )
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Move submodules to device
    model.device = device  # VERY IMPORTANT for internal logic
    model.mods.encoder.to(device)
    model.mods.decoder.to(device)
    model.mods.masknet.to(device)

    print(f"Encoder device: {next(model.mods.encoder.parameters()).device}")
    print(f"Decoder device: {next(model.mods.decoder.parameters()).device}")
    print(f"MaskNet device: {next(model.mods.masknet.parameters()).device}")
    return model, device

def load_waveform(path, target_sr=8000, device=None):
  assert device is not None
  waveform, sr = torchaudio.load(path)
  waveform = waveform.to(device)  # Move to the correct device

  if sr != target_sr:
      resampler = Resample(orig_freq=sr, new_freq=target_sr).to(device)  # Move resampler to correct device
      waveform = resampler(waveform)

  return waveform, target_sr

def separate_sources(model, device, waveform):
    waveform = waveform.to(device)  # Ensure waveform is on the same device
    #model.to(device)
    print(device)
    print(waveform.device)
    print(next(model.parameters()).device)
    # Just in case model wasn't moved yet
    #assert waveform.device == device, "Waveform and model must be on the same device!"
    waveform.device == torch.device("cuda:0"), f"Waveform is on {waveform.device}, but expected {device}"
    with torch.no_grad():
        est_sources = model(waveform)
    return est_sources

def save_references(folder_path, spk1_id, spk2_id, ref_spk1, ref_spk2, sr):
    ref_spk1 = ref_spk1.cpu()
    ref_spk2 = ref_spk2.cpu()
    torchaudio.save(os.path.join(folder_path, f"{spk1_id}_ref.wav"), ref_spk1, sr)
    torchaudio.save(os.path.join(folder_path, f"{spk2_id}_ref.wav"), ref_spk2, sr)

def compute_metrics(refs, ests, sample_rate):
    refs_np = refs.squeeze(0).cpu().numpy()
    ests_np = ests.squeeze(0).cpu().numpy().T

    min_len = min(refs_np.shape[-1], ests_np.shape[-1])
    refs_np = refs_np[:, :min_len]
    ests_np = ests_np[:, :min_len]

    sdr, sir, sar, perm = mir_eval.separation.bss_eval_sources(refs_np, ests_np)

    pesq_scores = []
    for i in range(2):
        ref_audio = refs[i].cpu().numpy()
        deg_audio = ests[0][i].cpu().numpy()

        min_len = int(sample_rate * 0.25)
        ref_audio = np.pad(ref_audio, (0, max(0, min_len - len(ref_audio))), 'constant')
        deg_audio = np.pad(deg_audio, (0, max(0, min_len - len(deg_audio))), 'constant')

        try:
            pesq_score = pesq(fs=sample_rate, ref=ref_audio, deg=deg_audio, mode='nb')
        except:
            pesq_score = np.nan

        pesq_scores.append(pesq_score)

    avg_pesq = np.nanmean(pesq_scores)
    return sdr, sir, sar, perm, pesq_scores, avg_pesq

# Process a single folder
def process_folder(model, device, root_dir, folder):
    folder_path = os.path.join(root_dir, folder)
    mixture_path = os.path.join(folder_path, f"{folder}.wav")

    if not os.path.exists(mixture_path):
        print(f"Skipping {folder} (mixture not found)")
        return None

    ids = folder.split("-")
    if len(ids) != 2:
        print(f"Skipping {folder} (invalid naming)")
        return None

    spk1_id, spk2_id = ids
    spk1_path = os.path.join(folder_path, f"{spk1_id}.wav")
    spk2_path = os.path.join(folder_path, f"{spk2_id}.wav")

    if not os.path.exists(spk1_path) or not os.path.exists(spk2_path):
        print(f"Skipping {folder} (reference files missing)")
        return None

    # mixture_waveform, sample_rate = load_waveform(mixture_path)
    # mixture_waveform = mixture_waveform.to(device)
    mixture_waveform, sample_rate = load_waveform(mixture_path, device=device)
    #mixture_waveform = mixture_waveform.to(device)
    print(f'Mixturewaveform{mixture_waveform.device}')
    est_sources = separate_sources(model, device, mixture_waveform)

    ref_spk1, _ = load_waveform(spk1_path, sample_rate, device)
    #ref_spk1= ref_spk1.to(device)
    ref_spk2, _ = load_waveform(spk2_path, sample_rate, device)
    #ref_spk2= ref_spk2.to(device)
    refs = torch.cat([ref_spk1, ref_spk2], dim=0)

    save_references(folder_path, spk1_id, spk2_id, ref_spk1, ref_spk2, sample_rate)
    sdr, sir, sar, perm, pesq_scores, avg_pesq = compute_metrics(refs, est_sources, sample_rate)

    return {
        "Mixture": folder,
        "SDR_1": sdr[0], "SDR_2": sdr[1],
        "SIR_1": sir[0], "SIR_2": sir[1],
        "SAR_1": sar[0], "SAR_2": sar[1],
        "Perm_1": perm[0], "Perm_2": perm[1],
        "PESQ_1": pesq_scores[0],
        "PESQ_2": pesq_scores[1],
        "Avg_PESQ": avg_pesq
    }

# Main loop to process all folders
def evaluate_all_folders(root_dir):
  model, device = load_model()
  all_metrics = []

  for folder in os.listdir(root_dir):
    folder_path = os.path.join(root_dir, folder)
    if os.path.isdir(folder_path):
      #print(f"Processing {folder}")
      metrics = process_folder(model, device, root_dir, folder)
      if metrics:
        all_metrics.append(metrics)

  df = pd.DataFrame(all_metrics)
  #print(df)
  df.to_csv(os.path.join(root_dir, "evaluation_metrics.csv"), index=False)
  return df

# Run the evaluation
root_dir = "/content/drive/My Drive/vox2_test_aac/mix_utterances/test"
df_metrics = evaluate_all_folders(root_dir)

"""# Rank-1 identification accuracy:Finetuned speaker identification model"""

import torch
import torchaudio
import librosa
from transformers import WavLMModel, Wav2Vec2FeatureExtractor
from peft import LoraConfig, get_peft_model

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load base WavLM model and feature extractor
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("microsoft/wavlm-base-plus")
base_model = WavLMModel.from_pretrained("microsoft/wavlm-base-plus").to(device)

# Reconfigure LoRA
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=["q_proj", "k_proj", "v_proj", "out_proj", "intermediate_dense", "output_dense"]
)

# Apply LoRA
model = get_peft_model(base_model, lora_config).to(device)

# Load fine-tuned weights
model.load_state_dict(torch.load("/content/drive/My Drive/fine_tuned_wavlm.pth", map_location=device))

# Set model to evaluation mode
model.eval()

def extract_embedding(audio_path):
    # Load and process audio
    waveform, _ = librosa.load(audio_path, sr=16000)

    # Ensure fixed length (pad/truncate)
    FIXED_AUDIO_LENGTH = 16000  # 1 sec at 16kHz
    if len(waveform) < FIXED_AUDIO_LENGTH:
        pad_length = FIXED_AUDIO_LENGTH - len(waveform)
        waveform = np.pad(waveform, (0, pad_length), mode='constant')
    else:
        waveform = waveform[:FIXED_AUDIO_LENGTH]

    # Convert to tensor and process with feature extractor
    inputs = feature_extractor(waveform, sampling_rate=16000, return_tensors="pt")
    inputs = inputs["input_values"].to(device)

    # Forward pass through the model
    with torch.no_grad():
        outputs = model(inputs)

    # Extract embeddings (mean pooling over time)
    embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()

    return embedding

import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
results = []
import os
for folder in os.listdir(test_folder):
    embedding_dict={}
    folder_path = os.path.join(test_folder, folder)
    wav_f=[os.path.join(folder_path, i)for i in os.listdir(folder_path) if not ('-' in i.split('/')[-1])]
    embedding_dict = {path.split('/')[-1]: extract_embedding(path) for path in wav_f}
    ref_f = [r for r in wav_f if '_ref' in r]
    cand_f = [j for j in list(set(wav_f) - set(ref_f)) if not ('-' in j.split('/')[-1])]
    for ref in ref_f:
      ref_emb = embedding_dict[ref.split('/')[-1]]
      max_score = -1
      best_match = None
      for cand in cand_f:
          cand_emb = embedding_dict[cand.split('/')[-1]]
          score = cosine_similarity(ref_emb.reshape(1, -1), cand_emb.reshape(1, -1))[0][0]
          if score > max_score:
              max_score = score
              best_match = cand
      # Store result
      results.append({
          'ref_id': ref.split('/')[-1],
          'best_match': best_match.split('/')[-1],
          #'is_correct': int(best_match.split('_')[0] == ref.split('_')[0])
      })

# Create dataframe
df_results = pd.DataFrame(results)
#rank1_accuracy = df_results['is_correct'].mean()

"""# Rank-1 identification : Pre-trained WaveLM Pulse Model"""

from transformers import WavLMModel, Wav2Vec2FeatureExtractor
from pathlib import Path

def load_audio(file_path, target_sample_rate=16000):
    """Load an audio file and resample to 16kHz if needed."""
    waveform, _ = librosa.load(file_path, sr=target_sample_rate)
    return torch.tensor(waveform, dtype=torch.float32).unsqueeze(0).to(device)  # Move audio tensor to GPU

def extract_embedding_pretrained(audio_path):
    """Extract speaker embedding from an audio file."""
    waveform = load_audio(audio_path)
    inputs = feature_extractor(waveform.squeeze(0).cpu().numpy(), sampling_rate=16000, return_tensors="pt", padding=True)

    # Move inputs to GPU
    inputs = {key: value.to(device) for key, value in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)

    # Use mean pooling on the last hidden state to get a fixed-length embedding
    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()  # Move tensor back to CPU before converting to numpy
    return embedding

import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import os
def evaluate_model(model_type, test_folder):
  results = []
  for folder in os.listdir(test_folder):
      embedding_dict={}
      folder_path = os.path.join(test_folder, folder)
      wav_f=[os.path.join(folder_path, i)for i in os.listdir(folder_path) if not ('-' in i.split('/')[-1])]
      if model_type == 'pretrained':
        embedding_dict = {path.split('/')[-1]: extract_embedding_pretrained(path) for path in wav_f}
      else:
        embedding_dict = {path.split('/')[-1]: extract_embedding(path) for path in wav_f}
      ref_f = [r for r in wav_f if '_ref' in r]
      cand_f = [j for j in list(set(wav_f) - set(ref_f)) if not ('-' in j.split('/')[-1])]
      for ref in ref_f:
        ref_emb = embedding_dict[ref.split('/')[-1]]
        max_score = -1
        best_match = None
        for cand in cand_f:
            cand_emb = embedding_dict[cand.split('/')[-1]]
            score = cosine_similarity(ref_emb.reshape(1, -1), cand_emb.reshape(1, -1))[0][0]
            if score > max_score:
                max_score = score
                best_match = cand
        # Store result
        results.append({
            'ref_id': ref.split('/')[-1],
            'best_match': best_match.split('/')[-1],
            #'is_correct': int(best_match.split('_')[0] == ref.split('_')[0])
        })

  # Create dataframe
  df_results = pd.DataFrame(results)
  return df_results
  #rank1_accuracy = df_results['is_correct'].mean()

df_results = evaluate_model('pretrained', test_folder = OUTPUT_DIR+'/test/')

print(df_results)

df_results_f = evaluate_model('fine-tuned', test_folder = OUTPUT_DIR+'/test/')

print(df_results_f)