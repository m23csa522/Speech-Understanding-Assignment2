# -*- coding: utf-8 -*-
"""Speech-Understanding-Assignment-2-II

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nvAIoS6vSc_7sD8G-gKI03WXxIhwhQMt
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import os
import numpy as np
import torch
import torchaudio
import librosa
import pickle
from transformers import WavLMModel, Wav2Vec2FeatureExtractor
from pathlib import Path

# Check for GPU availability
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load feature extractor and model
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("microsoft/wavlm-base-plus")
model = WavLMModel.from_pretrained("microsoft/wavlm-base-plus").to(device)  # Move model to GPU
model.eval()

def load_audio(file_path, target_sample_rate=16000):
    """Load an audio file and resample to 16kHz if needed."""
    waveform, _ = librosa.load(file_path, sr=target_sample_rate)
    return torch.tensor(waveform, dtype=torch.float32).unsqueeze(0).to(device)  # Move audio tensor to GPU

def extract_embedding(audio_path):
    """Extract speaker embedding from an audio file."""
    waveform = load_audio(audio_path)
    inputs = feature_extractor(waveform.squeeze(0).cpu().numpy(), sampling_rate=16000, return_tensors="pt", padding=True)

    # Move inputs to GPU
    inputs = {key: value.to(device) for key, value in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)

    # Use mean pooling on the last hidden state to get a fixed-length embedding
    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()  # Move tensor back to CPU before converting to numpy
    return embedding

base_path = "/content/drive/My Drive/vox1_test_wav/wav/"

def process_audio_files(audio_dir, output_pickle):
    """Process all audio files in a directory and save embeddings as a pickle file."""
    embeddings = {}
    count=0
    for audio_file in audio_dir:
      count+=1
      name = audio_file.replace(base_path, '')
      print(f"{name}:{count}")
      embeddings[name] = extract_embedding(str(audio_file))
    # Save embeddings to pickle file
    with open(output_pickle, "wb") as f:
        pickle.dump(embeddings, f)

df = pd.read_excel('/content/drive/My Drive/trial_pairs_vox1.xlsx')

audio_directory = list(set(df['audio_path_1'].tolist() + df['audio_path_2'].tolist()))
audio_directory = [f'{base_path}{i}' for i in audio_directory]

output_pickle_file = "/content/drive/My Drive/embeddings.pkl"
process_audio_files(audio_directory, output_pickle_file)

file_path = Path("/content/drive/My Drive/embeddings.pkl")  # Update with your file path

# Open the pickle file and load its contents
with open(file_path, "rb") as file:
    data = pickle.load(file)

import torch
import pandas as pd
import pickle

# Set device for GPU computation
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load embeddings from pickle file
with open("/content/drive/My Drive/embeddings.pkl", "rb") as f:
    embeddings = pickle.load(f)

# Function to calculate cosine similarity
def cosine_similarity(embedding1, embedding2):
  emb1 = torch.tensor(embedding1, dtype=torch.float32, device=device)
  emb2 = torch.tensor(embedding2, dtype=torch.float32, device=device)

  # Normalize embeddings
  emb1 = emb1 / emb1.norm(dim=-1, keepdim=True)
  emb2 = emb2 / emb2.norm(dim=-1, keepdim=True)
  # Compute cosine similarity
  similarity = torch.dot(emb1, emb2).item()
  return similarity

# Compute similarity for each pair
similarities = []
count=0
for _, row in df.iterrows():
  count+=1
  #print(count)
  emb1 = embeddings[row['audio_path_1']]
  emb2 = embeddings[row['audio_path_2']]
  similarity = cosine_similarity(emb1, emb2)
  similarities.append(similarity)

# Add similarity column to DataFrame
df["similarity"] = similarities

# Save updated DataFrame
df.to_excel("/content/drive/My Drive/trial_pairs_with_similarity.xlsx", index=False)

"""# Computing the metrics"""

import numpy as np
import pandas as pd
import torch
from sklearn.metrics import roc_curve, accuracy_score

def compute_speaker_metrics(df):
    """
    Compute EER (Equal Error Rate), TAR@1%FAR, and Speaker Identification Accuracy.

    Parameters:
        df_path (str): Path to the Excel file containing similarity scores and labels.

    Returns:
        dict: Dictionary containing EER, TAR@1%FAR, and Accuracy.
    """

    # Extract labels and similarity scores
    labels = df["label"].values  # 1 = Genuine pair, 0 = Impostor pair
    scores = df["similarity"].values  # Cosine similarity scores

    # Compute False Acceptance Rate (FAR), True Positive Rate (TPR), and thresholds
    fpr, tpr, thresholds = roc_curve(labels, scores)

    # Compute False Negative Rate (FNR)
    fnr = 1 - tpr

    # Find Equal Error Rate (EER)
    eer_index = np.nanargmin(np.abs(fpr - fnr))  # Find the index where FAR ≈ FRR
    eer_threshold = thresholds[eer_index]
    eer = fpr[eer_index] * 100  # Convert to percentage

    # Compute TAR @ 1% FAR (Find the TPR where FAR ≈ 1%)
    far_1_index = np.where(fpr <= 0.01)[0][-1]  # Last index where FAR <= 1%
    tar_at_1_far = tpr[far_1_index] * 100  # Convert to percentage

    # Compute Speaker Identification Accuracy using EER threshold
    predictions = (scores >= eer_threshold).astype(int)  # Classify based on EER threshold
    accuracy = accuracy_score(labels, predictions) * 100  # Convert to percentage

    # Return results as a dictionary
    return {
        "EER (%)": round(eer, 2),
        "TAR @ 1% FAR (%)": round(tar_at_1_far, 2),
        "Speaker Identification Accuracy (%)": round(accuracy, 2),
    }

# Usage Example
#df_path = "/content/drive/My Drive/trial_pairs_with_similarity.xlsx"
metrics = compute_speaker_metrics(df)

# Print Results
print(metrics)

"""# Fine Tunning"""

import torch
import torchaudio
import librosa
import os
import numpy as np
import pandas as pd
from transformers import WavLMModel, Wav2Vec2FeatureExtractor
from peft import LoraConfig, get_peft_model
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.optim as optim
from torch.nn import functional as F

# Set device for training
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load WavLM base model
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("microsoft/wavlm-base-plus")
base_model = WavLMModel.from_pretrained("microsoft/wavlm-base-plus").to(device)

# Configure LoRA
# Configure LoRA
lora_config = LoraConfig(
    r=8,  # Rank
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=["q_proj", "k_proj", "v_proj", "out_proj", "intermediate_dense", "output_dense"]
)

# Apply LoRA
model = get_peft_model(base_model, lora_config).to(device)
model.train()

# VoxCeleb2 dataset path
vox2_path = "/content/drive/My Drive/vox2_test_aac/aac"

# Collect identity folders (sorted)
identities = os.listdir(vox2_path)  # Sort to pick the first 100 for training

  # Next 18 for testing

import os

# Split data
train_ids = identities[:100]  # First 100 identities for training
test_ids = identities[100:]

train_dir = [os.path.join(vox2_path, id_folder, file)
               for id_folder in train_ids
               for file in os.listdir(os.path.join(vox2_path, id_folder))
               ]

test_dir = [os.path.join(vox2_path, id_folder, file)
              for id_folder in test_ids
              for file in os.listdir(os.path.join(vox2_path, id_folder))
              ]

test_files=[]
for path in test_dir:
  temp=[f"{path}/{i}" for i in os.listdir(path)]
  test_files+=temp

train_files=[]
for path in train_dir:
  temp=[f"{path}/{i}" for i in os.listdir(path)]
  train_files+=temp

# Define a fixed audio length (in samples) for consistency
FIXED_AUDIO_LENGTH = 16000  # 1 second at 16kHz

# Create Speaker Dataset
class SpeakerDataset(Dataset):
    def __init__(self, file_paths, label_map):
        self.file_paths = file_paths
        self.label_map = label_map

    def __len__(self):
        return len(self.file_paths)

    def __getitem__(self, idx):
        file_path = self.file_paths[idx]
        label = self.label_map[file_path.split('/')[-3]]

        waveform, _ = librosa.load(file_path, sr=16000)

        # Ensure fixed length: pad or truncate
        if len(waveform) < FIXED_AUDIO_LENGTH:
            pad_length = FIXED_AUDIO_LENGTH - len(waveform)
            waveform = np.pad(waveform, (0, pad_length), mode='constant')
        else:
            waveform = waveform[:FIXED_AUDIO_LENGTH]

        inputs = feature_extractor(waveform, sampling_rate=16000, return_tensors="pt")

        return inputs["input_values"].squeeze(0), torch.tensor(label, dtype=torch.long)

# Create label mapping
label_map = {identity: idx for idx, identity in enumerate(train_ids)}

# Load datasets
train_dataset = SpeakerDataset(train_files, label_map)
test_dataset = SpeakerDataset(test_files, label_map)

# Custom Collate Function for DataLoader
def collate_fn(batch):
    """Pads audio inputs to the longest sequence in batch."""
    inputs, labels = zip(*batch)
    inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0.0)
    labels = torch.stack(labels)
    return inputs.to(device), labels.to(device)

# DataLoader
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

# Define ArcFace Loss
class ArcFaceLoss(nn.Module):
    def __init__(self, embedding_size, num_classes, margin=0.5, scale=64):
        super().__init__()
        self.W = nn.Parameter(torch.randn(embedding_size, num_classes).to(device))
        self.margin = margin
        self.scale = scale

    def forward(self, embeddings, labels):
        norm_W = F.normalize(self.W, dim=0)
        norm_embeddings = F.normalize(embeddings, dim=1)

        cosine = torch.matmul(norm_embeddings, norm_W)
        one_hot = F.one_hot(labels, num_classes=self.W.shape[1])

        margin_cosine = cosine - one_hot * self.margin
        return F.cross_entropy(self.scale * margin_cosine, labels)

# Initialize loss and optimizer
embedding_dim = base_model.config.hidden_size  # WavLM hidden size
num_speakers = len(train_ids)

criterion = ArcFaceLoss(embedding_dim, num_speakers).to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-4)

# Training Loop
epochs = 10
for epoch in range(epochs):
    model.train()
    total_loss = 0

    for inputs, labels in train_loader:
        optimizer.zero_grad()

        # Move inputs and labels to the same device as the model
        inputs = inputs.to(device)
        labels = labels.to(device)

        with torch.no_grad():
            outputs = model(inputs)

        embeddings = outputs.last_hidden_state.mean(dim=1)
        loss = criterion(embeddings, labels)

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}")

# Save model
torch.save(model.state_dict(), "/content/drive/My Drive/fine_tuned_wavlm.pth")

"""# Extracting Embeddings from fine-tuned Model


"""

import torch
import torchaudio
import librosa
from transformers import WavLMModel, Wav2Vec2FeatureExtractor
from peft import LoraConfig, get_peft_model

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load base WavLM model and feature extractor
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("microsoft/wavlm-base-plus")
base_model = WavLMModel.from_pretrained("microsoft/wavlm-base-plus").to(device)

# Reconfigure LoRA
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=["q_proj", "k_proj", "v_proj", "out_proj", "intermediate_dense", "output_dense"]
)

# Apply LoRA
model = get_peft_model(base_model, lora_config).to(device)

# Load fine-tuned weights
model.load_state_dict(torch.load("/content/drive/My Drive/fine_tuned_wavlm.pth", map_location=device))

# Set model to evaluation mode
model.eval()

def extract_embedding(audio_path):
    # Load and process audio
    waveform, _ = librosa.load(audio_path, sr=16000)

    # Ensure fixed length (pad/truncate)
    FIXED_AUDIO_LENGTH = 16000  # 1 sec at 16kHz
    if len(waveform) < FIXED_AUDIO_LENGTH:
        pad_length = FIXED_AUDIO_LENGTH - len(waveform)
        waveform = np.pad(waveform, (0, pad_length), mode='constant')
    else:
        waveform = waveform[:FIXED_AUDIO_LENGTH]

    # Convert to tensor and process with feature extractor
    inputs = feature_extractor(waveform, sampling_rate=16000, return_tensors="pt")
    inputs = inputs["input_values"].to(device)

    # Forward pass through the model
    with torch.no_grad():
        outputs = model(inputs)

    # Extract embeddings (mean pooling over time)
    embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()

    return embedding

fine_tuned_embeddings = np.vstack([extract_embedding(f) for f in audio_directory])
print("Embeddings shape:", fine_tuned_embeddings.shape)

substring_to_remove = "/content/drive/My Drive/vox1_test_wav/wav/"
fine_tuned_embedding_dic={}
for i in range(len(audio_directory)):
  fine_tuned_embedding_dic[audio_directory[i].replace(substring_to_remove, "")]=fine_tuned_embeddings[i]

import pickle
with open("/content/drive/My Drive/fine_tuned_embeddings_dictionary.pkl", "wb") as f:  # Changed 'rb' to 'wb' to open in write mode
    pickle.dump(fine_tuned_embedding_dic, f)

c=[i.split('/')[0] for i in fine_tuned_embedding_dic.keys()]

# Compute similarity for each pair
similarities = []
count=0
for _, row in df.iterrows():
  count+=1
  print(count)
  emb1 = fine_tuned_embedding_dic[row['audio_path_1']]
  emb2 = fine_tuned_embedding_dic[row['audio_path_2']]
  similarity = cosine_similarity(emb1, emb2)
  similarities.append(similarity)

df['similarity_fine_tuned']=similarities

print(compute_speaker_metrics(df.drop('similarity', axis=1).rename({'similarity_fine_tuned':'similarity'}, axis=1)))

"""#"""

